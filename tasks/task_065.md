# Task: ```python
import duckdb

con = duckdb.connect()

# Transform numeric array by adding 10 to each element
rel = con.query('''
SELECT array_transform([1, 2, 3, 4], x -> x + 10) as incremented_array
''')

print(rel.execute().fetchall())
```

This targeted implementation demonstrates DuckDB's array_transform capability by incrementing list elements, highlighting its flexible functional SQL querying approach.

## Step 1: ```python

**Generated by**: Anthropic

```python
import duckdb

# Create an in-memory connection
conn = duckdb.connect(':memory:')

# Create a sample sales table
conn.execute('''
    CREATE TABLE sales (
        product_id INTEGER,
        sale_date DATE,
        amount DECIMAL(10,2)
    );

    INSERT INTO sales VALUES
        (1, '2023-01-15', 150.50),
        (1, '2023-02-20', 200.75),
        (2, '2023-01-10', 100.25),
        (2, '2023-03-05', 175.80)
''');

# Use window function to calculate cumulative sales per product
result = conn.execute('''
    SELECT 
        product_id, 
        sale_date, 
        amount,
        SUM(amount) OVER (PARTITION BY product_id ORDER BY sale_date) as cumulative_sales
    FROM sales
''').fetchall()

for row in result:
    print(row)
```

**Result**: [(1, datetime.date(2023, 1, 15), Decimal('150.50'), Decimal('150.50')), (1, datetime.date(2023, 2, 20), Decimal('200.75'), Decimal('351.25')), (2, datetime.date(2023, 1, 10), Decimal('100.25'), Decimal('100.25')), (2, datetime.date(2023, 3, 5), Decimal('175.80'), Decimal('276.05'))]

**Explanation**: Demonstrates a window function (cumulative sales) using a sales dataset, showing how to calculate running totals partitioned by product with DuckDB
## Step 2: import duckdb

**Generated by**: Anthropic

```python
import duckdb

# Create an in-memory database connection
conn = duckdb.connect(':memory:')

# Create a sample products table
conn.execute('''
    CREATE TABLE products (
        product_id INTEGER PRIMARY KEY,
        product_name VARCHAR,
        category VARCHAR,
        price DECIMAL(10,2)
    );

    INSERT INTO products VALUES
        (1, 'Laptop', 'Electronics', 1200.00),
        (2, 'Smartphone', 'Electronics', 800.50),
        (3, 'Headphones', 'Electronics', 150.25)
''');

# Demonstrate aggregate and filtering capabilities
result = conn.execute('''
    SELECT 
        category, 
        COUNT(*) as product_count,
        AVG(price) as avg_price
    FROM products
    WHERE category = 'Electronics'
    GROUP BY category
''').fetchall()

for row in result:
    print(f"Category: {row[0]}, Products: {row[1]}, Average Price: ${row[2]:.2f}")
```

**Result**: [('Electronics', 3, 716.9166666666666)]

**Explanation**: Demonstrates DuckDB's table creation, data insertion, and basic SQL aggregation with filtering, showing how to use in-memory databases for quick data analysis
## Step 3: con = duckdb.connect()

**Generated by**: Anthropic

```python
import duckdb

# Create in-memory connection
conn = duckdb.connect(':memory:')

# Create a time series table with sensor data
conn.execute('''
    CREATE TABLE sensor_readings (
        timestamp TIMESTAMP,
        sensor_id INTEGER,
        temperature FLOAT
    );

    INSERT INTO sensor_readings VALUES
        ('2023-06-01 10:00:00', 1, 22.5),
        ('2023-06-01 11:00:00', 1, 23.1),
        ('2023-06-01 10:00:00', 2, 21.8),
        ('2023-06-01 11:00:00', 2, 22.3)
''');

# Compute rolling average temperature per sensor
result = conn.execute('''
    SELECT
        sensor_id,
        timestamp,
        temperature,
        AVG(temperature) OVER (
            PARTITION BY sensor_id
            ORDER BY timestamp
            ROWS BETWEEN 1 PRECEDING AND CURRENT ROW
        ) as rolling_avg
    FROM sensor_readings
''').fetchall()

for row in result:
    print(f"Sensor {row[0]}: {row[1]} - Temp: {row[2]}, Rolling Avg: {row[3]:.2f}")
```

**Result**: [(1, datetime.datetime(2023, 6, 1, 10, 0), 22.5, 22.5), (1, datetime.datetime(2023, 6, 1, 11, 0), 23.100000381469727, 22.800000190734863), (2, datetime.datetime(2023, 6, 1, 10, 0), 21.799999237060547, 21.799999237060547), (2, datetime.datetime(2023, 6, 1, 11, 0), 22.299999237060547, 22.049999237060547)]

**Explanation**: Demonstrates DuckDB's window function for computing a rolling average of temperature readings across multiple sensors using a time-windowed approach.
## Step 4: # Transform numeric array by adding 10 to each element

**Generated by**: Anthropic

```python
import duckdb

conn = duckdb.connect(':memory:')

# Create array to transform
array_data = [1, 2, 3, 4, 5]

# Use DuckDB array transformation
result = conn.execute('SELECT array_transform([1, 2, 3, 4, 5], x -> x + 10) as transformed_array').fetchone()[0]

print(result)  # Should output: [11, 12, 13, 14, 15]
```

**Result**: [11, 12, 13, 14, 15]

**Explanation**: Demonstrates DuckDB's array_transform function for applying a lambda function to each array element, adding 10 to each numeric value
## Step 5: rel = con.query('''

**Generated by**: Anthropic

```python
import duckdb

# Create in-memory connection
conn = duckdb.connect(':memory:')

# Create a complex hierarchical data table
conn.execute('''
    CREATE TABLE organizational_hierarchy (
        employee_id INTEGER PRIMARY KEY,
        name VARCHAR,
        department VARCHAR,
        manager_id INTEGER
    );

    INSERT INTO organizational_hierarchy VALUES
        (1, 'Alice', 'HR', NULL),
        (2, 'Bob', 'Sales', 1),
        (3, 'Charlie', 'Marketing', 1),
        (4, 'David', 'Sales', 2);
''');

# Use recursive common table expression to traverse hierarchy
result = conn.execute('''
    WITH RECURSIVE hierarchy_traversal AS (
        SELECT employee_id, name, department, manager_id, 0 as depth
        FROM organizational_hierarchy
        WHERE manager_id IS NULL

        UNION ALL

        SELECT 
            o.employee_id, 
            o.name, 
            o.department, 
            o.manager_id, 
            h.depth + 1
        FROM organizational_hierarchy o
        JOIN hierarchy_traversal h ON o.manager_id = h.employee_id
    )
    SELECT name, department, depth
    FROM hierarchy_traversal
    ORDER BY depth, name
''').fetchall()

for row in result:
    print(f"Name: {row[0]}, Department: {row[1]}, Hierarchy Depth: {row[2]})")
```

**Result**: [('Alice', 'HR', 0), ('Bob', 'Sales', 1), ('Charlie', 'Marketing', 1), ('David', 'Sales', 2)]

**Explanation**: Demonstrates recursive CTE for organizational hierarchy traversal, showing complex hierarchical data querying with depth tracking and employee relationships
## Step 6: SELECT array_transform([1, 2, 3, 4], x -> x + 10) as incremented_array

**Generated by**: Anthropic

```python
import duckdb

conn = duckdb.connect(':memory:')
result = conn.execute('SELECT array_transform([1, 2, 3, 4], x -> x + 10) as incremented_array').fetchone()[0]
print(result)
```

**Result**: [11, 12, 13, 14]

**Explanation**: Demonstrates DuckDB's array_transform function, which applies a lambda transformation to each element of an array. In this example, it adds 10 to each input value.
## Step 7: ''')

**Generated by**: Anthropic

```python
import duckdb

conn = duckdb.connect(':memory:')

# Create a custom probability function
conn.create_function('prob_calc', lambda x: x * 0.5, [duckdb.type('FLOAT')], duckdb.type('FLOAT'))

# Use the custom function in a query
result = conn.execute('SELECT prob_calc(10.0) as probability').fetchone()[0]

print(f'Probability: {result}')
```

**Result**: 5.0

**Explanation**: Demonstrates creating a custom scalar function in DuckDB that calculates a probability by multiplying input by 0.5, showcasing the extensibility of DuckDB's function system
## Step 8: print(rel.execute().fetchall())

**Generated by**: Anthropic

```python
import duckdb

conn = duckdb.connect(':memory:')

# Create sample table
conn.execute('CREATE TABLE numbers (val INTEGER)')
conn.execute('INSERT INTO numbers VALUES (1), (2), (3), (4), (5)')

# Create relational query
rel = conn.table('numbers').filter('val > 2')
print(rel.execute().fetchall())
```

**Result**: 5.0

**Explanation**: Demonstrates creating a relational algebra query with DuckDB by filtering a table and executing the result using the relational API
## Step 9: ```

**Generated by**: Anthropic

```python
import duckdb

conn = duckdb.connect(':memory:')

# Create time series data with weather patterns
conn.execute('''
    CREATE TABLE weather_data (
        date DATE,
        city VARCHAR,
        temperature FLOAT,
        precipitation FLOAT
    );

    INSERT INTO weather_data VALUES
        ('2023-06-01', 'New York', 75.5, 0.2),
        ('2023-06-02', 'New York', 78.3, 0.0),
        ('2023-06-01', 'Chicago', 68.7, 0.5),
        ('2023-06-02', 'Chicago', 72.1, 0.1)
''');

# Use window function to track temperature changes
result = conn.execute('''
    SELECT 
        city, 
        date, 
        temperature,
        temperature - LAG(temperature) OVER (PARTITION BY city ORDER BY date) as temp_change
    FROM weather_data
''').fetchall()

for row in result:
    print(f"City: {row[0]}, Date: {row[1]}, Temperature: {row[2]}, Temperature Change: {row[3] or 'N/A'}")
```

**Result**: [('Chicago', datetime.date(2023, 6, 1), 68.69999694824219, None), ('Chicago', datetime.date(2023, 6, 2), 72.0999984741211, 3.4000015258789062), ('New York', datetime.date(2023, 6, 1), 75.5, None), ('New York', datetime.date(2023, 6, 2), 78.30000305175781, 2.8000030517578125)]

**Explanation**: Demonstrates window function LAG() to calculate temperature changes across time series data, showing DuckDB's ability to perform time-based comparative analytics on structured datasets
## Step 10: This targeted implementation demonstrates DuckDB's array_transform capability by incrementing list elements, highlighting its flexible functional SQL querying approach.

**Generated by**: Anthropic

```python
import duckdb

conn = duckdb.connect(':memory:')

# Define array and apply transformation
result = conn.execute('SELECT array_transform([5, 10, 15, 20], x -> x * 2) as doubled_array').fetchone()[0]

print(result)
```

**Result**: [10, 20, 30, 40]

**Explanation**: Demonstrates DuckDB's array_transform function by doubling each element in an input array. Shows how SQL can apply functional transformations directly on array data types.
